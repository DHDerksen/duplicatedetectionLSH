{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPq-WokN3C8g"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "import json\n",
        "import numpy as np\n",
        "from itertools import combinations\n",
        "import sympy as sy\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import re\n",
        "import matplotlib.colors as mcolors\n",
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "random.seed(11)\n",
        "\n",
        "# Loads the JSON data into a Python dictionary\n",
        "with open('TVs-all-merged.json') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Separates the elements\n",
        "data_reshape = {}\n",
        "i = 1\n",
        "for key in data.keys():\n",
        "    for description in data[key]:\n",
        "        data_reshape[i] = description\n",
        "        i+=1\n",
        "\n",
        "def cleaning_steps(title):\n",
        "    \"\"\"\n",
        "    Takes a title as input and cleans it according to the specified steps\n",
        "    \"\"\"\n",
        "    # Convert title to lowercase\n",
        "    title = title.lower()\n",
        "    # Remove characters that are not lowercase letters, numbers, spaces, '.', or '\"'\n",
        "    title = re.sub(r'[^a-z0-9\\s\\.\\\"]', '', title)\n",
        "    # Replace variations of 'inches' and 'hz' with standardised forms ('inch' and 'hz')\n",
        "    title = re.sub(r' ?inches?| ?inch| ?\"| ?hertzs?| ?hz', lambda match: 'inch' if 'inch' in match.group(0) or '\"' in match.group(0) else 'hz', title)\n",
        "    return title\n",
        "\n",
        "def extract_title_words(data, new_title, regex):\n",
        "    \"\"\"\n",
        "    Extracts the model words, specified by the regex function, from the titles\n",
        "    \"\"\"\n",
        "    for key, title in new_title.items():\n",
        "        # Use regular expression to find all matches in the title\n",
        "        extracted = re.findall(regex, title)\n",
        "        # Flatten the nested list and remove leading/trailing whitespaces\n",
        "        flattened = [word for group in extracted for word in group if word.strip()]\n",
        "        new_title[key] = list(set(flattened))\n",
        "\n",
        "def append_product_color(data, new_title, colorinclusion):\n",
        "    \"\"\"\n",
        "    Appends the color to the list of model words, if that information exists\n",
        "    \"\"\"\n",
        "    if colorinclusion:\n",
        "        for key, product_data in data.items():\n",
        "            color_keys = ['Color:', 'Cabinet Color:']  # Keys of interest\n",
        "            for color_key in color_keys:\n",
        "                # Only append color if the product contains that key-value pair. Clean in similar fashion to title cleaning.\n",
        "                if color_key in product_data['featuresMap']:\n",
        "                    product_color = product_data['featuresMap'][color_key]\n",
        "                    product_color_cleaned = re.sub(\"[^a-zA-Z0-9\\s\\.]\", \"\", product_color.lower())\n",
        "                    new_title[key] += [product_color_cleaned] # Append color value as a list item\n",
        "\n",
        "def append_brand_info(data, new_title, includebrandname):\n",
        "    \"\"\"\n",
        "    Appends the brand to the list of model words, if that information exists\n",
        "    \"\"\"\n",
        "    if includebrandname:\n",
        "        for key, product_data in data.items():\n",
        "            brand_keys = ['Brand:', 'Brand Name:']  # Keys of interest\n",
        "            for brand_key in brand_keys:\n",
        "                # Only append brand if the product contains that key-value pair. Clean in similar fashion to title cleaning.\n",
        "                if brand_key in product_data['featuresMap']:\n",
        "                    brand_value = product_data['featuresMap'][brand_key]\n",
        "                    brand_value_cleaned = re.sub(\"[^a-zA-Z0-9\\s\\.]\", \"\", brand_value.lower())\n",
        "                    new_title[key] += [brand_value_cleaned]  # Append brand value as a list item\n",
        "\n",
        "\n",
        "def clean_titles(data, includebrandname, includecolors):\n",
        "    \"\"\"\n",
        "    Combines above methods consecutively in order to clean titles, find model words using the prev_regex pattern and append color and brand information\n",
        "    \"\"\"\n",
        "    set_titles = {key: data[key]['title'] for key in data.keys()}\n",
        "    # Previous regex pattern for finding model words\n",
        "    prev_regex = r'[a-zA-Z0-9]*[a-zA-Z]+[0-9]+[a-zA-Z0-9]*|[a-zA-Z0-9]*[0-9]+[a-zA-Z]+[a-zA-Z0-9]*|[0-9]+[.][0-9]+[a-zA-Z]*|[0-9]+'\n",
        "\n",
        "    # All recognized color names from mcolors\n",
        "    all_colors = '|'.join(mcolors.CSS4_COLORS.keys())\n",
        "\n",
        "    if includecolors:\n",
        "      # Combine the regex patterns\n",
        "      regex = f'({prev_regex})|({all_colors})'\n",
        "    else:\n",
        "      regex = prev_regex\n",
        "\n",
        "    new_title = set_titles.copy()\n",
        "\n",
        "    # Consecutive execution of above methods\n",
        "    for key in new_title.keys():\n",
        "        new_title[key] = cleaning_steps(new_title[key])\n",
        "\n",
        "    extract_title_words(data, new_title, regex)\n",
        "    append_product_color(data, new_title, includecolors)\n",
        "    append_brand_info(data, new_title, includebrandname)\n",
        "    return new_title\n",
        "\n",
        "def create_binary_matrix(data, min_word_frequency):\n",
        "    \"\"\"\n",
        "    Creates the binary vector representation for each product on what descriptive words they contain. Vectors are combined in a matrix.\n",
        "    \"\"\"\n",
        "\n",
        "    # Extract unique words from the dataset\n",
        "    unique_words = sorted(set(word for value_list in data.values() for word in value_list))\n",
        "\n",
        "    # Create a binary matrix where values are set to 1 if descriptive word is present in the title or feature map\n",
        "    bin_matrix = np.zeros((len(unique_words), len(data)), dtype=int)\n",
        "\n",
        "    for col_idx, value_list in enumerate(data.values()):\n",
        "        for row_idx, word in enumerate(unique_words):\n",
        "            if word in value_list:\n",
        "                bin_matrix[row_idx, col_idx] = 1\n",
        "\n",
        "    # Function used to count the number of times a word occures over all titles\n",
        "    wordfreq = np.sum(bin_matrix, axis=1)\n",
        "\n",
        "    # Filter out rows (words) whose sum is not higher than min_word_frequency\n",
        "    mask = wordfreq >= min_word_frequency\n",
        "    filtered_binmatrix = bin_matrix[mask]\n",
        "    filtered_unique_words = np.array(unique_words)[mask]\n",
        "\n",
        "    # Print dimensions before and after filtering\n",
        "    print(f\"Dimensions before applying word frequency filtering: {bin_matrix.shape}\")\n",
        "    print(f\"Dimensions after applying word frequency filtering: {filtered_binmatrix.shape}\")\n",
        "\n",
        "    return filtered_binmatrix\n",
        "\n",
        "def generate_hashvalues(lengthbinmatrix, numHashes):\n",
        "    \"\"\"\n",
        "    Creates hash values by permuting the rows of the above created binary matrix. This is done numHashes times.\n",
        "    \"\"\"\n",
        "    hash_values = []\n",
        "    for _ in range(numHashes):\n",
        "        hash_index = np.random.permutation(lengthbinmatrix) + 1  # Generate random permutations of lengthBinary\n",
        "        dictionary = {i: hash_index[i] for i in range(lengthbinmatrix)}  # Create a dictionary manually mapping indices to permuted indices\n",
        "        hash_values.append(dictionary)\n",
        "    return hash_values\n",
        "\n",
        "def generate_signatures(binary_matrix, hash_values, num_hashes):\n",
        "    \"\"\"\n",
        "    Generates signatures from the above created hash matrix.\n",
        "    \"\"\"\n",
        "    all_signatures = []\n",
        "\n",
        "    # Number of products being compared\n",
        "    num_products = binary_matrix.shape[1]\n",
        "\n",
        "    for product_idx in range(num_products):\n",
        "        product_signature = []\n",
        "\n",
        "        for hash_idx in range(num_hashes):\n",
        "            hash_val = 0  # Initialize hash value\n",
        "\n",
        "            while hash_val < len(binary_matrix):\n",
        "                index = hash_values[hash_idx].get(hash_val + 1)  # Add 1 to hash value\n",
        "                if index is not None:\n",
        "                    if binary_matrix[index - 1, product_idx] == 1:\n",
        "                        product_signature.append(hash_val + 1)\n",
        "                        break\n",
        "                hash_val += 1\n",
        "\n",
        "        all_signatures.append(product_signature)\n",
        "\n",
        "    return all_signatures\n",
        "\n",
        "def hash_signatures_to_buckets(products, product_signatures, b):\n",
        "    \"\"\"\n",
        "    Hashes product signatures into buckets for Locality-Sensitive Hashing (LSH).\n",
        "    \"\"\"\n",
        "    n_signatures = len(product_signatures[0])\n",
        "    if n_signatures % b != 0:\n",
        "        raise ValueError(\"The number of signatures cannot be divided by the number of bands!\")\n",
        "\n",
        "    r = int(n_signatures / b)\n",
        "    product_keys = list(products.keys())  # Get product keys for indexing\n",
        "\n",
        "    # Initialize bucket_bands as a list of dictionaries\n",
        "    bucket_bands = [{} for _ in range(b)]\n",
        "\n",
        "    signature_index = 0\n",
        "\n",
        "    # Go through all signatures\n",
        "    for signature in product_signatures:\n",
        "        bands = signature_to_bands(signature, r)\n",
        "\n",
        "        # Go through each band i of a signature\n",
        "        for i, band in enumerate(bands.astype(str)):\n",
        "            band_string = ','.join(band)\n",
        "            if band_string not in bucket_bands[i]:\n",
        "                bucket_bands[i][band_string] = []\n",
        "            bucket_bands[i][band_string].append(product_keys[signature_index])\n",
        "        signature_index += 1\n",
        "\n",
        "    return bucket_bands  # Return the list of dictionaries with buckets for each band\n",
        "\n",
        "\n",
        "def signature_to_bands(signature, rows_per_band):\n",
        "    \"\"\"\n",
        "    Converts a signature into bands for LSH.\n",
        "    \"\"\"\n",
        "    bands = []\n",
        "    remainder = len(signature) % rows_per_band\n",
        "    for i in range(0, len(signature) - remainder, rows_per_band):  # Step size: rows_per_band\n",
        "        bands.append(signature[i:i + rows_per_band])\n",
        "    # If there's a remainder, pad the last band with zeros\n",
        "    if remainder > 0:\n",
        "        last_band = signature[-remainder:] + [0] * (rows_per_band - remainder)\n",
        "        bands.append(last_band)\n",
        "    return np.array(bands)\n",
        "\n",
        "def find_candidate_pairs(bucket_bands):\n",
        "    \"\"\"\n",
        "    Finds candidate pairs from buckets obtained through LSH.\n",
        "    \"\"\"\n",
        "    all_candidates = []\n",
        "    # Iterate through bands and their corresponding buckets\n",
        "    for band_idx, band_buckets in enumerate(bucket_bands):\n",
        "        for bucket_key in band_buckets:\n",
        "            products_in_bucket = band_buckets[bucket_key]\n",
        "            if len(products_in_bucket) > 1:\n",
        "                pairs = generate_pairs(products_in_bucket)\n",
        "                all_candidates.extend(pairs)\n",
        "\n",
        "    # Remove duplicates by converting the list to a set and then back to a list\n",
        "    unique_candidates = list(set(all_candidates))\n",
        "\n",
        "    return unique_candidates\n",
        "\n",
        "def generate_pairs(products):\n",
        "    \"\"\"\n",
        "    Generates unique pairs from a list of products.\n",
        "    \"\"\"\n",
        "    num_products = len(products)\n",
        "    pairs = []\n",
        "    for i in range(num_products):\n",
        "        for j in range(i + 1, num_products):\n",
        "            pairs.append((products[i], products[j]))\n",
        "    return pairs\n",
        "\n",
        "def MSM(candidate_pairs, titles, similarity_treshold, q, w):\n",
        "    \"\"\"\n",
        "    Perform Multi-component Similarity Measure (MSM) to identify final pairs from candidate pairs. A weighted average between Q-grams and Jaccard is calculated.\n",
        "    \"\"\"\n",
        "    final_pairs = []\n",
        "\n",
        "    def generate_qgrams(titles, q):\n",
        "        qgrams = []\n",
        "        for i in range(len(titles) - q + 1):\n",
        "            qgrams.append(tuple(titles[i:i + q]))  # Convert the list of q-grams into tuples\n",
        "        return qgrams\n",
        "\n",
        "    for product1, product2 in candidate_pairs:\n",
        "        # Calculate Q-gram similarity between two titles\n",
        "        title1_qgrams = generate_qgrams(titles[product1], q)\n",
        "        title2_qgrams = generate_qgrams(titles[product2], q)\n",
        "\n",
        "        intersection = len(set(title1_qgrams).intersection(set(title2_qgrams)))\n",
        "        union = len(set(title1_qgrams).union(set(title2_qgrams)))\n",
        "\n",
        "        qgram_similarity = intersection / union if union != 0 else 0\n",
        "\n",
        "        # Calculate Jaccard similarity between two titles\n",
        "        jaccard_intersection = len(set(titles[product1]).intersection(set(titles[product2])))\n",
        "        jaccard_union = len(set(titles[product1]).union(set(titles[product2])))\n",
        "\n",
        "        jaccard_similarity = jaccard_intersection / jaccard_union if jaccard_union != 0 else 0\n",
        "\n",
        "        # Weighted sum of Q-gram and Jaccard similarity\n",
        "        similarity_score = (w * qgram_similarity) + ((1 - w) * jaccard_similarity)\n",
        "\n",
        "        if similarity_score > similarity_treshold:\n",
        "            final_pairs.append((product1, product2))\n",
        "\n",
        "    return final_pairs\n",
        "\n",
        "\n",
        "def sample_data(data, n_samples):\n",
        "    \"\"\"\n",
        "    Randomly samples a specified number of items from a dictionary.\n",
        "    \"\"\"\n",
        "    sample_dict = {}\n",
        "    for i in range(1, n_samples+1):\n",
        "        draw = random.randint(1, len(data))\n",
        "        sample_dict[i] = data[draw]\n",
        "    return sample_dict\n",
        "\n",
        "def true_positives(candidate_pairs, dataset):\n",
        "    \"\"\"\n",
        "    Calculates the number of true positives from a list of candidate pairs based on modelID.\n",
        "    \"\"\"\n",
        "    TP = 0\n",
        "    for candidate_pair in candidate_pairs:\n",
        "        if dataset[candidate_pair[0]]['modelID'] == dataset[candidate_pair[1]]['modelID']:\n",
        "            TP += 1\n",
        "    return TP\n",
        "\n",
        "def total_num_duplicates(dataset, cleaned_titles):\n",
        "    \"\"\"\n",
        "    Identifies and returns all pairs of indices corresponding to duplicate products based on modelID.\n",
        "    \"\"\"\n",
        "    model_ids = {}\n",
        "    for key, data in dataset.items():\n",
        "        model_ids[key] = data['modelID']\n",
        "\n",
        "    model_dict = defaultdict(set)\n",
        "    for key, model_id in model_ids.items():\n",
        "        model_dict[model_id].add(key)\n",
        "\n",
        "    duplicates = [pair for indices in model_dict.values() if len(indices) > 1 for pair in combinations(indices, 2)]\n",
        "\n",
        "    return duplicates\n",
        "\n",
        "def sort_sets(candidate_pairs):\n",
        "    return {(min(pair), max(pair)) for pair in candidate_pairs}\n",
        "\n",
        "def computeMeasures(duplicates, candidate_pairs, found_pairs):\n",
        "    \"\"\"\n",
        "    Computes evaluation measures (PQ, PC, F1*, F1) for duplicate detection based on candidate pairs and found pairs.\n",
        "    \"\"\"\n",
        "    duplicates = sort_sets(duplicates)\n",
        "    candidate_pairs = sort_sets(candidate_pairs)\n",
        "    found_pairs = sort_sets(found_pairs)\n",
        "\n",
        "    duplicates = set(duplicates)\n",
        "    total_duplicates = len(duplicates)\n",
        "\n",
        "    candidate_pairs = set(candidate_pairs)\n",
        "    found_pairs = set(found_pairs)\n",
        "\n",
        "    # Measures MSM\n",
        "    TP = len(found_pairs.intersection(duplicates))\n",
        "    FP = len(found_pairs) - TP\n",
        "    FN = total_duplicates - TP\n",
        "\n",
        "    # Metrics\n",
        "    PQ = TP / len(candidate_pairs) if len(found_pairs) != 0 else 0\n",
        "    PC = TP / total_duplicates if total_duplicates != 0 else 0\n",
        "\n",
        "    if PQ + PC != 0:\n",
        "        F1_star = (2 * PQ * PC) / (PQ + PC)\n",
        "    else:\n",
        "        F1_star = 0\n",
        "\n",
        "    if TP + FP != 0 and TP + FN != 0:\n",
        "        precision = TP / (TP + FP)\n",
        "        recall = TP / (TP + FN)\n",
        "\n",
        "        if recall + precision != 0:\n",
        "            F1 = (2 * recall * precision) / (recall + precision)\n",
        "        else:\n",
        "            F1 = 0\n",
        "    else:\n",
        "        F1 = 0\n",
        "\n",
        "    return PQ, PC, F1_star, F1\n",
        "\n",
        "def compute_metrics(data, data_reshape, bandwidths, bootstraps, n, sim_tres, q, w, min_word_frequency):\n",
        "    \"\"\"\n",
        "    Performs duplicate detection and computes metrics over multiple iterations with bootstrapping.\n",
        "    \"\"\"\n",
        "    iter_count = 0\n",
        "    measures = None\n",
        "    while iter_count != bootstraps:\n",
        "        # Sample data\n",
        "        n_samples = int(len(data_reshape) * 0.63)\n",
        "        data_subset = sample_data(data_reshape, n_samples)\n",
        "\n",
        "        # Create titles\n",
        "        cleaned_titles = clean_titles(data_subset, includebrandname=True, includecolors=True)\n",
        "\n",
        "        # Start minhashing\n",
        "        binary_matrix = create_binary_matrix(cleaned_titles, min_word_frequency)\n",
        "        hashValues = generate_hashvalues(len(binary_matrix), n)\n",
        "        signatures = generate_signatures(binary_matrix,hashValues, n)\n",
        "\n",
        "        duplicates = total_num_duplicates(data_subset, cleaned_titles)\n",
        "\n",
        "        # Metric values for all band values for the current iteration\n",
        "        PQ_i = []\n",
        "        PC_i = []\n",
        "        F1star_i = []\n",
        "        F1_i = []\n",
        "        foc_i = []\n",
        "\n",
        "        for bandwidth in bandwidths:\n",
        "            # Apply LSH\n",
        "            bands = int(bandwidth[0])\n",
        "            bucket_bands = hash_signatures_to_buckets(cleaned_titles, signatures, bands)\n",
        "            candidate_pairs = find_candidate_pairs(bucket_bands)\n",
        "\n",
        "            # Apply MSM\n",
        "            found_pairs = MSM(candidate_pairs, cleaned_titles, sim_tres, q, w)\n",
        "\n",
        "            # Compute measures\n",
        "            PQ, PC, F1_star, F1 = computeMeasures(duplicates, candidate_pairs, found_pairs)\n",
        "            fraction_of_comparison = len(candidate_pairs) / len(list(combinations(data_subset.keys(), 2)))\n",
        "            #print(\"PQ = \", PQ, \"PC = \", PC, \"F1_star = \", F1_star, \"F1 = \", F1)\n",
        "            PQ_i.append(PQ)\n",
        "            PC_i.append(PC)\n",
        "            F1star_i.append(F1_star)\n",
        "            F1_i.append(F1)\n",
        "            foc_i.append(fraction_of_comparison)\n",
        "\n",
        "        if iter_count == 0:\n",
        "            measures = np.stack((PQ_i, PC_i, F1star_i, F1_i, foc_i), 0)\n",
        "        else:\n",
        "            measures += np.stack((PQ_i, PC_i, F1star_i, F1_i, foc_i), 0)\n",
        "\n",
        "        iter_count += 1\n",
        "\n",
        "    # Averaged measures with rows=(PQ, PC, F1star, F1) and columns=|settings|\n",
        "    final_measures = measures / bootstraps\n",
        "    # Replace the print statements with these lines to generate plots\n",
        "    plt.figure(figsize=(10, 8))\n",
        "\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.plot(final_measures[4], final_measures[0])\n",
        "    plt.xlabel('Fraction of Comparisons')\n",
        "    plt.ylabel('PQ')\n",
        "    plt.title('PQ vs Fraction of Comparisons')\n",
        "\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.plot(final_measures[4], final_measures[1])\n",
        "    plt.xlabel('Fraction of Comparisons')\n",
        "    plt.ylabel('PC')\n",
        "    plt.title('PC vs Fraction of Comparisons')\n",
        "\n",
        "    plt.subplot(2, 2, 3)\n",
        "    plt.plot(final_measures[4], final_measures[2])\n",
        "    plt.xlabel('Fraction of Comparisons')\n",
        "    plt.ylabel('F1*')\n",
        "    plt.title('F1* vs Fraction of Comparisons')\n",
        "\n",
        "    plt.subplot(2, 2, 4)\n",
        "    plt.plot(final_measures[4], final_measures[3])\n",
        "    plt.xlabel('Fraction of Comparisons')\n",
        "    plt.ylabel('F1')\n",
        "    plt.title('F1 vs Fraction of Comparisons')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    return final_measures\n",
        "\n",
        "# Initialisation\n",
        "bootstraps = 5\n",
        "n = 720\n",
        "q = 3\n",
        "min_word_frequency = 0\n",
        "\n",
        "bandwidths = []\n",
        "\n",
        "for r in [1,2,3,4,5,8,10,20,30,60,72,90]:\n",
        "    if (n % r) == 0:\n",
        "        b = n / r\n",
        "        bandwidths.append([b, r])\n",
        "\n",
        "# Define lists of values for similarity_threshold and w for manual grid search\n",
        "similarity_threshold_values = [0.4, 0.5, 0.6]\n",
        "w_values = [0.25, 0.5, 0.75]\n",
        "\n",
        "# Loop over different combinations of similarity_threshold and w\n",
        "for similarity_threshold in similarity_threshold_values:\n",
        "    for w in w_values:\n",
        "        # Call compute_metrics function with varying similarity_threshold and w\n",
        "        print(\"Weight used = \", w, \"similarity theshold used = \", similarity_threshold)\n",
        "        result = compute_metrics(data, data_reshape, bandwidths, bootstraps, n, similarity_threshold, q, w, min_word_frequency)"
      ]
    }
  ]
}